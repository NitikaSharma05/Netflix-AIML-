{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "qBMux9mC6MCf",
        "yiiVWRdJDDil",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** Nitika\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project titled “Unsupervised ML – Netflix Movies and TV Shows Clustering” focuses on analyzing and clustering the dataset of TV shows and movies available on Netflix as of 2019. The dataset was collected from Flixable, a third-party Netflix search engine that has been a reliable source for Netflix content information. The broader goal of the project is to apply exploratory data analysis (EDA) and unsupervised machine learning techniques to derive meaningful insights about Netflix’s content catalog and trends over time.\n",
        "\n",
        "The business context highlights the rapid changes that Netflix’s catalog has undergone in the past decade. According to a 2018 report, the number of TV shows on Netflix has nearly tripled since 2010. While Netflix has significantly increased its investment in TV shows, its number of movies has declined, with more than 2,000 titles disappearing since 2010. This shift suggests a strategic emphasis on television content, reflecting Netflix’s evolving focus to retain global audiences through long-form and episodic formats. With the number of TV shows rising dramatically and the count of movies dropping, it becomes valuable to investigate what other trends and patterns can be extracted from the dataset.\n",
        "\n",
        "The dataset provides opportunities for deep insights not only in terms of raw counts but also by connecting with external datasets such as IMDB ratings or Rotten Tomatoes scores. Such integration can uncover further dimensions like audience preferences, critical reception, and regional variations. For instance, linking Netflix’s content with IMDB ratings might reveal whether Netflix has been more successful in promoting high-rated TV shows versus movies. Rotten Tomatoes data could help in identifying patterns of critical versus audience approval across genres and countries.\n",
        "\n",
        "The project requires performing Exploratory Data Analysis (EDA) to understand the dataset’s structure, distributions, and hidden patterns. EDA helps answer questions like: Which countries produce the most Netflix content? What genres are most represented? How has the balance between movies and TV shows shifted over time? By plotting trends, distributions, and relationships, EDA builds the foundation for subsequent modeling.\n",
        "\n",
        "Another key objective is to understand what type of content is available in different countries. Netflix operates in a global market, and its strategy often varies by region. Certain markets might have a stronger representation of localized content, while others may feature more international or English-language productions. By examining the geographic spread of movies and TV shows, one can analyze how Netflix balances global hits with regional diversity.\n",
        "\n",
        "The project also emphasizes investigating whether Netflix has increasingly focused on TV shows rather than movies in recent years. This can be validated by comparing annual additions of movies and shows to the platform, looking for consistent upward or downward trends. If the data confirms the report’s findings, it would illustrate Netflix’s pivot toward serialized storytelling, possibly due to higher engagement levels and subscriber retention associated with TV shows.\n",
        "\n",
        "The central machine learning task in this project is clustering similar content by matching text-based features. Using natural language processing (NLP) techniques on metadata such as descriptions, genres, and keywords, the project aims to group similar titles together. For example, clustering could identify groups of crime dramas, romantic comedies, or family-oriented shows, irrespective of country or release year. Such clustering would not only reveal patterns in Netflix’s catalog but also provide insights for recommendation systems and content strategy.\n",
        "\n",
        "In summary, this project combines exploratory analysis, trend identification, regional comparisons, and unsupervised learning methods to study Netflix’s evolving catalog. By doing so, it sheds light on how Netflix has restructured its library over time, how its focus differs across countries, and how unsupervised clustering can group content for better understanding. The integration of external datasets like IMDB and Rotten Tomatoes further enhances the analysis, making it possible to assess content quality alongside quantity. The insights derived can serve as valuable inputs for both academic exploration and real-world business strategies in streaming content."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NitikaSharma05/Netflix-AIML-/blob/main/README.md"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rapid expansion of Netflix’s content library has resulted in significant shifts in its catalog composition over the past decade, with a notable decline in movies and a sharp rise in TV shows. While the volume of content is large and diverse, there is limited structured understanding of how titles can be categorized, compared, and analyzed systematically. Without effective analysis, it is difficult to identify viewing trends, regional content preferences, or the strategic emphasis on different types of media.\n",
        "\n",
        "This project aims to address this gap by applying unsupervised machine learning techniques and exploratory data analysis on Netflix’s dataset of movies and TV shows. The goal is to uncover hidden patterns, understand global and regional content availability, validate the shift in focus from movies to TV shows, and cluster similar titles using text-based features such as genres and descriptions. By doing so, the project seeks to provide actionable insights into Netflix’s evolving content strategy and demonstrate how unsupervised learning can be leveraged for media analytics and recommendation systems."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n",
        "\n",
        "# Display shape and first 5 rows\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "print(\"\\nSample Data:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, cols = df.shape\n",
        "print(\"Number of Rows:\", rows)\n",
        "print(\"Number of Columns:\", cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(\"Number of Duplicate Rows:\", duplicate_count)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains information about Netflix movies and TV shows, including attributes such as title, type (Movie or TV Show), director, cast, country, release year, rating, duration, and genre. It provides both categorical and text-based features, making it suitable for exploratory data analysis, trend identification, and clustering tasks. The data spans multiple countries and years, highlighting the diversity of Netflix’s catalog, though it also contains missing values in fields like director, cast, and country, which will require preprocessing. Overall, it is a rich dataset for analyzing global content distribution and patterns on Netflix.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "columns = df.columns.tolist()\n",
        "print(\"Columns:\", columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset includes variables that describe different aspects of Netflix content: **show_id** serves as a unique identifier for each title, **type** specifies whether it is a Movie or TV Show, **title** gives the name of the content, **director** and **cast** list the creative contributors, **country** indicates where the title was produced, **date_added** shows when it was made available on Netflix, **release_year** refers to the original release year, **rating** represents the maturity classification, **duration** provides either the runtime in minutes or number of seasons, **listed_in** categorizes the title into genres, and **description** contains a short text summary of the content.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "print(\"Unique Values for Each Variable:\")\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Dataset Analysis Ready\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce', format='mixed')\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df['day_added'] = df['date_added'].dt.day\n",
        "\n",
        "df['director'] = df['director'].fillna('Unknown')\n",
        "df['cast'] = df['cast'].fillna('Unknown')\n",
        "df['country'] = df['country'].fillna('Unknown')\n",
        "df['rating'] = df['rating'].fillna('Unknown')\n",
        "df['duration'] = df['duration'].fillna('Unknown')\n",
        "\n",
        "df['listed_in'] = df['listed_in'].str.strip()\n",
        "df['description'] = df['description'].str.strip()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converted `date_added` to datetime and extracted year, month, day. Filled missing values in director, cast, country, rating, duration with \"Unknown\". Stripped spaces in genres and descriptions. Insights: dataset had many missing values in creative fields, most shows were added after 2015, TV shows grew faster than movies.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Chart - 1 : Count of Movies vs TV Shows\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='type', palette='Set2')\n",
        "plt.title(\"Number of Movies vs TV Shows on Netflix\")\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is the simplest way to compare two categories (Movies vs TV Shows).\n",
        "\n",
        "The chart shows that Netflix has more Movies than TV Shows.\n",
        "\n",
        "Positive impact: helps understand catalog balance. Negative: if TV shows are fewer, retention may drop since series drive longer engagement."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chose bar chart because it clearly highlights category distribution at a glance.\n",
        "\n",
        "Insight: TV Shows have grown steadily but still lag in total count compared to Movies.\n",
        "\n",
        "Positive: informs strategy to invest more in shows. Negative: overinvestment in movies may reduce binge-watch potential."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used this chart as it effectively visualizes frequency counts for categorical variables.\n",
        "\n",
        "Insight: Movies dominate the dataset, but TV Shows are catching up in recent years.\n",
        "\n",
        "Positive: guides content diversification decisions. Negative: dominance of movies may slow subscriber growth where serialized content is preferred."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Chart - 2 : Top 10 Countries Producing Netflix Content\n",
        "plt.figure(figsize=(10,6))\n",
        "country_data = df['country'].value_counts().head(10)\n",
        "sns.barplot(x=country_data.values, y=country_data.index, palette='viridis')\n",
        "plt.title(\"Top 10 Countries by Number of Netflix Titles\")\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chose a horizontal bar chart because it effectively shows country-wise comparisons and handles long labels better."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: The US dominates Netflix content, followed by India and the UK, showing strong regional contributions."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix identify markets with strong production and target investments. Negative: Heavy US dominance may limit global diversity, reducing growth in non-English-speaking regions."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Chart - 3 : Number of Titles Added Over the Years\n",
        "plt.figure(figsize=(12,6))\n",
        "df['year_added'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Number of Titles Added to Netflix Over the Years\")\n",
        "plt.xlabel(\"Year Added\")\n",
        "plt.ylabel(\"Count of Titles\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a bar chart because it clearly shows year-over-year growth trends of content added."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Netflix’s content library expanded rapidly after 2015, peaking around 2018–2019."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Indicates strong growth and aggressive expansion strategy. Negative: Sudden slowdown or drop in later years may signal saturation or reduced investments, potentially impacting subscriber growth."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Chart - 4 : Distribution of Content Ratings\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(data=df, y='rating', order=df['rating'].value_counts().index, palette='coolwarm')\n",
        "plt.title(\"Distribution of Content Ratings on Netflix\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a horizontal countplot since it clearly compares frequency of different maturity ratings."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Most content falls under TV-MA and TV-14, showing Netflix’s focus on mature and teen audiences."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix tailor recommendations and marketing by knowing audience segments. Negative: Overemphasis on mature ratings may limit appeal for children/family viewers, reducing growth in that demographic."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Chart - 5 : Top 10 Genres on Netflix\n",
        "plt.figure(figsize=(12,6))\n",
        "genre_data = df['listed_in'].str.split(',').explode().str.strip().value_counts().head(10)\n",
        "sns.barplot(x=genre_data.values, y=genre_data.index, palette='magma')\n",
        "plt.title(\"Top 10 Genres on Netflix\")\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Genre\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a bar chart because it effectively shows categorical distribution and highlights most frequent genres."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Genres like International Movies, Dramas, and Comedies dominate Netflix’s catalog, showing their global appeal."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Guides Netflix to strengthen popular genres and attract more viewers. Negative: Overconcentration in a few genres may cause lack of variety, leading to reduced growth among niche audiences."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Chart - 6 : Movies Duration Distribution\n",
        "movies = df[df['type'] == 'Movie']\n",
        "movies['duration_num'] = movies['duration'].str.replace(' min','').astype(str).str.extract('(\\d+)').astype(float)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(movies['duration_num'].dropna(), bins=30, kde=True, color='teal')\n",
        "plt.title(\"Distribution of Movie Durations on Netflix\")\n",
        "plt.xlabel(\"Duration (minutes)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a histogram with KDE to visualize how movie lengths are distributed across the catalog."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Most movies fall between 80–120 minutes, with fewer very short or very long films."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix align new productions with audience-preferred durations. Negative: Lack of variety in duration (too standardized) may reduce appeal for viewers who prefer short films or epic-length features."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Chart - 7 : TV Shows by Number of Seasons\n",
        "tv_shows = df[df['type'] == 'TV Show'].copy() # Create a copy to avoid SettingWithCopyWarning\n",
        "tv_shows['seasons_num'] = tv_shows['duration'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(data=tv_shows, x='seasons_num', palette='Spectral',\n",
        "              order=tv_shows['seasons_num'].value_counts().index)\n",
        "plt.title(\"Distribution of TV Shows by Number of Seasons\")\n",
        "plt.xlabel(\"Number of Seasons\")\n",
        "plt.ylabel(\"Count of TV Shows\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a countplot to clearly show how many TV shows fall into each season-length category."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Most TV shows have only 1 or 2 seasons, with very few long-running series."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Indicates Netflix invests in limited series, reducing risk and production cost. Negative: Lack of long-running series may affect audience loyalty, as multi-season shows often drive long-term subscriptions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Chart - 8 : Word Cloud of Content Descriptions\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "text = \" \".join(df['description'].dropna().astype(str))\n",
        "wordcloud = WordCloud(width=1200, height=600, background_color='black',\n",
        "                      stopwords=STOPWORDS, colormap='plasma').generate(text)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Common Words in Netflix Content Descriptions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a word cloud because it visually highlights the most frequent keywords in text descriptions, making themes easy to spot."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Common words like “love,” “life,” “family,” and “story” dominate, showing Netflix’s emphasis on relatable, human-centered storytelling."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix tailor marketing around themes audiences connect with. Negative: Repetitive themes may signal lack of originality, which could reduce long-term viewer interest."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Chart - 9 : Heatmap of Content Type vs Rating\n",
        "plt.figure(figsize=(10,6))\n",
        "heatmap_data = df.pivot_table(index='rating', columns='type', values='show_id', aggfunc='count').fillna(0)\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='YlGnBu')\n",
        "plt.title(\"Heatmap of Ratings by Content Type\")\n",
        "plt.xlabel(\"Content Type\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a heatmap because it allows comparison of rating distribution across Movies and TV Shows simultaneously."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Movies dominate in general ratings, while TV shows are concentrated in mature categories like TV-MA and TV-14."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix align maturity levels with target audience preferences. Negative: Strong tilt toward mature ratings may reduce appeal for family-friendly or children’s markets, limiting growth in those segments."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Chart - 10 : Trend of Movies vs TV Shows Added Over Time\n",
        "plt.figure(figsize=(12,6))\n",
        "trend_data = df.groupby(['year_added','type'])['show_id'].count().reset_index()\n",
        "sns.lineplot(data=trend_data, x='year_added', y='show_id', hue='type', marker='o')\n",
        "plt.title(\"Trend of Movies vs TV Shows Added Over the Years\")\n",
        "plt.xlabel(\"Year Added\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.legend(title=\"Content Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a line chart because it best visualizes trends over time and compares Movies vs TV Shows growth patterns."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Movies were consistently dominant until recent years, where TV Shows saw a sharp rise, narrowing the gap."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix evaluate its pivot toward TV Shows and validate engagement strategies. Negative: If TV Shows grow too fast at the cost of movies, it may alienate movie-focused audiences, leading to churn in that segment."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Chart - 11 : Top 10 Directors with Most Titles on Netflix\n",
        "plt.figure(figsize=(12,6))\n",
        "director_data = df[df['director'] != 'Unknown']['director'].value_counts().head(10)\n",
        "sns.barplot(x=director_data.values, y=director_data.index, palette='cividis')\n",
        "plt.title(\"Top 10 Directors with Most Netflix Titles\")\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Director\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a bar chart because it clearly highlights which directors contribute most titles to Netflix."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: A small set of directors have multiple works on Netflix, while most others appear only once."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix strengthen collaborations with proven directors. Negative: Over-reliance on a few directors may limit creative diversity, potentially reducing content variety for viewers."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Chart - 12 : Monthly Trend of Content Added\n",
        "plt.figure(figsize=(12,6))\n",
        "month_data = df['month_added'].value_counts().sort_index()\n",
        "sns.lineplot(x=month_data.index, y=month_data.values, marker='o', color='crimson')\n",
        "plt.title(\"Monthly Trend of Content Added on Netflix\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Titles Added\")\n",
        "plt.xticks(range(1,13))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a line chart because it shows seasonality and monthly variation in content additions."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Certain months, like July and December, have higher additions, possibly aligning with holidays and viewer demand peaks."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix optimize release schedules for maximum impact. Negative: If releases are too clustered in specific months, other periods may feel content-poor, reducing consistent engagement."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Chart - 13 : Top 10 Actors/Actresses with Most Appearances on Netflix\n",
        "plt.figure(figsize=(12,6))\n",
        "cast_data = df[df['cast'] != 'Unknown']['cast'].str.split(',').explode().str.strip().value_counts().head(10)\n",
        "sns.barplot(x=cast_data.values, y=cast_data.index, palette='inferno')\n",
        "plt.title(\"Top 10 Actors/Actresses on Netflix\")\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Actor/Actress\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a bar chart because it efficiently shows the top actors/actresses by frequency and allows easy ranking comparison."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: A few actors dominate Netflix appearances, showing Netflix’s recurring partnerships with certain popular stars."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Helps Netflix identify bankable talent for future productions. Negative: Overuse of the same actors could create monotony, reducing novelty and audience excitement."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "numeric_df = df.select_dtypes(include=['int64','float64'])\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Picked a heatmap because it provides a clear visual summary of correlations between numeric variables in the dataset."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Weak or no strong correlations exist among most numeric variables (like year_added, release_year, duration), indicating they represent independent aspects of the data."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Pair Plot Visualization\n",
        "sns.pairplot(df.select_dtypes(include=['int64','float64']))\n",
        "plt.suptitle(\"Pair Plot of Numeric Features\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the pair plot because it allows visualization of relationships between multiple numeric variables at once, while also showing distributions of each variable. It’s a powerful tool to detect patterns, clusters, and potential correlations."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diagonal plots show the distribution of each numeric variable, helping spot skewness or outliers.\n",
        "\n",
        "The scatter plots between pairs highlight whether any variables show positive/negative correlation or no relation.\n",
        "\n",
        "In this dataset, numeric variables like release_year and duration may not strongly correlate, confirming they represent different aspects of the data."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Step 1: Define 3 Hypothetical Statements**\n",
        "\n",
        "1. **Hypothesis 1:** The average duration of movies is greater than the average duration of TV Shows.\n",
        "2. **Hypothesis 2:** The distribution of ratings is independent of the type of content (Movie vs TV Show).\n",
        "3. **Hypothesis 3:** There is a significant difference in the number of releases before 2015 and after 2015.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Hypothesis Testing with Code**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Example assumptions: df contains ['type', 'duration', 'rating', 'release_year']\n",
        "\n",
        "# -------------------------\n",
        "# Hypothesis 1: Duration\n",
        "# -------------------------\n",
        "movies = df[df['type'] == 'Movie']['duration'].dropna()\n",
        "tv_shows = df[df['type'] == 'TV Show']['duration'].dropna()\n",
        "\n",
        "t_stat, p_val = stats.ttest_ind(movies, tv_shows, equal_var=False)\n",
        "\n",
        "print(\"Hypothesis 1 - Avg Duration Movie vs TV Show\")\n",
        "print(\"t-statistic:\", t_stat, \"p-value:\", p_val)\n",
        "if p_val < 0.05:\n",
        "    print(\"Conclusion: Significant difference between Movie and TV Show duration.\")\n",
        "else:\n",
        "    print(\"Conclusion: No significant difference found.\")\n",
        "\n",
        "# -------------------------\n",
        "# Hypothesis 2: Ratings Independence\n",
        "# -------------------------\n",
        "contingency = pd.crosstab(df['type'], df['rating'])\n",
        "chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n",
        "\n",
        "print(\"\\nHypothesis 2 - Rating vs Type\")\n",
        "print(\"Chi-square:\", chi2, \"p-value:\", p_val)\n",
        "if p_val < 0.05:\n",
        "    print(\"Conclusion: Ratings are dependent on Type of content.\")\n",
        "else:\n",
        "    print(\"Conclusion: Ratings are independent of Type of content.\")\n",
        "\n",
        "# -------------------------\n",
        "# Hypothesis 3: Release Year Distribution\n",
        "# -------------------------\n",
        "before_2015 = df[df['release_year'] < 2015].shape[0]\n",
        "after_2015 = df[df['release_year'] >= 2015].shape[0]\n",
        "\n",
        "# Chi-square goodness-of-fit test\n",
        "observed = [before_2015, after_2015]\n",
        "expected = [sum(observed)/2, sum(observed)/2]\n",
        "\n",
        "chi2, p_val = stats.chisquare(observed, f_exp=expected)\n",
        "\n",
        "print(\"\\nHypothesis 3 - Releases Before vs After 2015\")\n",
        "print(\"Chi-square:\", chi2, \"p-value:\", p_val)\n",
        "if p_val < 0.05:\n",
        "    print(\"Conclusion: Significant difference in release counts before and after 2015.\")\n",
        "else:\n",
        "    print(\"Conclusion: No significant difference in releases across years.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Insights from the Tests**\n",
        "\n",
        "* **Hypothesis 1 (Duration):** If p-value < 0.05 → Movies have significantly longer average duration than TV Shows.\n",
        "* **Hypothesis 2 (Ratings):** If p-value < 0.05 → Ratings distribution is not uniform across type (e.g., Movies may get PG/PG-13 more often, Shows get TV-MA).\n",
        "* **Hypothesis 3 (Release Years):** If p-value < 0.05 → Netflix added significantly more content after 2015 than before, showing expansion.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average duration of Movies and TV Shows.\n",
        "(µ_movies = µ_tvshows)\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average duration of Movies and TV Shows.\n",
        "(µ_movies ≠ µ_tvshows)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming dataset already loaded as df with 'type' and 'duration' columns\n",
        "movies_duration = df[df['type'] == 'Movie']['duration'].str.extract('(\\d+)').dropna().astype(float)\n",
        "tv_shows_duration = df[df['type'] == 'TV Show']['duration'].str.extract('(\\d+)').dropna().astype(float)\n",
        "\n",
        "# Independent t-test (Welch’s t-test)\n",
        "t_stat, p_val = stats.ttest_ind(movies_duration, tv_shows_duration, equal_var=False)\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_val)\n",
        "\n",
        "# Decision rule\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "    print(\"Reject Null Hypothesis: Significant difference exists between Movies and TV Shows duration.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference found.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Independent Two-Sample t-test (Welch’s t-test)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e5c471"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4958589b"
      },
      "source": [
        "I performed an independent samples t-test (specifically, Welch's t-test, which does not assume equal variances)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bb02fee"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7622e4b7"
      },
      "source": [
        "I chose the independent samples t-test because I wanted to compare the means of a continuous variable (duration) between two independent groups (Movies and TV Shows). Welch's t-test was used because I did not assume that the variances of the two groups were equal."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing means of two independent groups (Movies vs TV Shows).\n",
        "\n",
        "The data (duration) is continuous.\n",
        "\n",
        "The groups are independent, not paired.\n",
        "\n",
        "Welch’s t-test is robust when variances between groups are not equal, which often happens in real datasets."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no association between content type (Movie/TV Show) and rating.\n",
        "(Content type and rating are independent)\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is an association between content type and rating.\n",
        "(Content type and rating are not independent)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Create contingency table for content type vs rating\n",
        "contingency_table = pd.crosstab(df['type'], df['rating'])\n",
        "\n",
        "# Chi-square test of independence\n",
        "chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-square Statistic:\", chi2_stat)\n",
        "print(\"p-value:\", p_val)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "\n",
        "# Decision Rule\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "    print(\"Reject Null Hypothesis: Significant association exists between content type and rating.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant association found.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed a Chi-Square Test of Independence."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data involves categorical variables:\n",
        "\n",
        "type (Movie, TV Show)\n",
        "\n",
        "rating (TV-MA, PG, R, etc.)\n",
        "\n",
        "Chi-square is the best statistical method to test association/independence between two categorical variables.\n",
        "\n",
        "It does not assume normal distribution and works directly on frequency counts."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the number of titles released before 2015 and after 2015.\n",
        "(µ_before = µ_after)\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the number of titles released before 2015 and after 2015.\n",
        "(µ_before ≠ µ_after)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Ensure date_added column is datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# Extract release year\n",
        "df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce')\n",
        "\n",
        "# Split into before 2015 and after 2015\n",
        "before_2015 = df[df['release_year'] < 2015]['release_year'].dropna()\n",
        "after_2015 = df[df['release_year'] >= 2015]['release_year'].dropna()\n",
        "\n",
        "# Independent t-test (Welch’s t-test)\n",
        "t_stat, p_val = stats.ttest_ind(before_2015, after_2015, equal_var=False)\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_val)\n",
        "\n",
        "# Decision rule\n",
        "alpha = 0.05\n",
        "if p_val < alpha:\n",
        "    print(\"Reject Null Hypothesis: Significant difference exists in releases before vs after 2015.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference found.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Independent Two-Sample t-test (Welch’s t-test)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing two independent groups: titles released before 2015 vs after 2015.\n",
        "\n",
        "The variable release_year is numerical (continuous).\n",
        "\n",
        "Groups are not paired (independent).\n",
        "\n",
        "Welch’s t-test is preferred because it does not assume equal variance."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Checking missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Impute categorical columns with mode (most frequent value)\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "# Impute numeric columns with median\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "for col in numeric_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mode Imputation for Categorical Variables – Columns like rating, country, and director were filled using the mode (most frequent value). This preserves the most common category without introducing artificial bias.\n",
        "\n",
        "Median Imputation for Numeric Variables – Columns like release_year or duration were filled using the median. Median is less sensitive to outliers compared to mean and gives a more robust central value.\n",
        "\n",
        "These techniques were chosen because they are simple, effective for large datasets, and ensure minimal distortion of the data distribution while keeping the dataset analysis-ready."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Example numeric column for outlier treatment (Duration)\n",
        "# Convert duration to numeric minutes where possible\n",
        "df['duration_num'] = df['duration'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "# Function to cap outliers using IQR method\n",
        "def cap_outliers(series):\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return np.where(series < lower_bound, lower_bound,\n",
        "                    np.where(series > upper_bound, upper_bound, series))\n",
        "\n",
        "# Apply outlier capping on duration_num\n",
        "df['duration_num'] = cap_outliers(df['duration_num'])\n",
        "\n",
        "# Verify treatment\n",
        "print(df['duration_num'].describe())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used IQR-based outlier treatment (capping) for handling outliers:\n",
        "\n",
        "IQR Method (Interquartile Range) – I calculated the lower and upper bounds using Q1 - 1.5IQR and Q3 + 1.5IQR. Any values beyond these thresholds were capped at the boundary. This prevents extreme values from skewing the analysis.\n",
        "\n",
        "Capping instead of removal – Instead of dropping rows, I replaced extreme values with boundary values. This ensures we don’t lose valuable data points and keeps dataset size consistent.\n",
        "\n",
        "This technique was chosen because it is simple, statistically robust, and works well in datasets like Netflix where outliers (e.g., very high movie durations) may exist due to rare but valid cases."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Encode your categorical columns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply Label Encoding for all categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Label Encoding for categorical columns:\n",
        "\n",
        "Label Encoding – Each unique category in categorical columns (e.g., type, rating, country) was assigned an integer value. This was chosen because the dataset is large and has multiple categorical features, and Label Encoding provides a simple way to convert them into numerical format for clustering and ML algorithms.\n",
        "\n",
        "Reason for choice – One-Hot Encoding would create too many new columns (especially for country or cast), leading to sparsity and inefficiency. Label Encoding is more memory-efficient and sufficient for models that can handle categorical values as integers.\n",
        "\n",
        "This technique ensures the dataset is fully numeric, making it ready for statistical analysis and machine learning without unnecessary dimensionality growth."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from contractions import contractions_dict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Work on a text column, e.g., \"description\"\n",
        "text_data = df['description'].astype(str)\n",
        "\n",
        "# Expand Contraction\n",
        "def expand_contractions(text):\n",
        "    for word, expanded in contractions_dict.items():\n",
        "        text = re.sub(r\"\\b\" + word + r\"\\b\", expanded, text)\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = text_data.apply(expand_contractions)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "700e8266"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['clean_text'] = df['clean_text'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits - Removed this step as it might be too aggressive\n",
        "# df['clean_text'] = df['clean_text'].apply(lambda x: re.sub(r'http\\S+|www\\S+', '', x))\n",
        "# df['clean_text'] = df['clean_text'].apply(lambda x: re.sub(r'\\w*\\d\\w*', '', x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords - Removed this step as it might be too aggressive and cause issues with vocabulary\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text - Removed this step as it might be too aggressive and cause issues with vocabulary\n",
        "from nltk.corpus import wordnet\n",
        "def synonym_replacement(text):\n",
        "     words = text.split()\n",
        "     new_words = []\n",
        "     for word in words:\n",
        "         synonyms = wordnet.synsets(word)\n",
        "         if synonyms:\n",
        "             new_words.append(synonyms[0].lemmas()[0].name())  # replace with a synonym\n",
        "         else:\n",
        "             new_words.append(word)\n",
        "     return \" \".join(new_words)\n",
        "\n",
        "# df['clean_text'] = df['clean_text'].apply(synonym_replacement) # This line was causing the indentation error"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization - Removed this step for now\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "df['tokens'] = df['clean_text'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['stemmed_tokens'] = df['tokens'].apply(lambda x: [ps.stem(word) for word in x])\n",
        "df['lemmatized_tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used both stemming and lemmatization. Stemming reduces words to their root by trimming suffixes (faster, but less accurate). Lemmatization converts words to their dictionary form using linguistic rules (slower, but more precise). Lemmatization was emphasized because it preserves meaningful word forms, which is important in descriptive text like movie summaries."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76d31660"
      },
      "source": [
        "#df['pos_tags'] = df['tokens'].apply(pos_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "source": [
        "# POS Taging - Removed this step for now\n",
        "# nltk.download('averaged_perceptron_tagger_eng')\n",
        "# df['pos_tags'] = df['tokens'].apply(pos_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used TF-IDF Vectorization. Unlike simple Bag-of-Words, TF-IDF gives weight to unique words while reducing the importance of very frequent but less meaningful words. This is more effective for clustering or content-based recommendation tasks in the Netflix dataset since it highlights distinctive terms in movie/TV descriptions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check column names\n",
        "print(\"Available columns:\", df.columns)\n",
        "\n",
        "# If 'date_added' exists, convert it; otherwise skip\n",
        "if 'date_added' in df.columns:\n",
        "    df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "    df['year_added'] = df['date_added'].dt.year\n",
        "    df['month_added'] = df['date_added'].dt.month\n",
        "    df['day_added'] = df['date_added'].dt.day\n",
        "else:\n",
        "    df['year_added'] = None\n",
        "    df['month_added'] = None\n",
        "    df['day_added'] = None\n",
        "\n",
        "# Extract duration in numeric form (if column exists)\n",
        "if 'duration' in df.columns:\n",
        "    df['duration_num'] = df['duration'].str.extract('(\\d+)').astype(float)\n",
        "else:\n",
        "    df['duration_num'] = None\n",
        "\n",
        "# Create content age (if release_year exists)\n",
        "if 'release_year' in df.columns and 'year_added' in df.columns:\n",
        "    df['content_age'] = df['year_added'] - df['release_year']\n",
        "else:\n",
        "    df['content_age'] = None\n",
        "\n",
        "# Simplify rating into broader categories\n",
        "def simplify_rating(r):\n",
        "    if r in ['PG', 'TV-PG', 'TV-Y', 'TV-Y7']:\n",
        "        return 'Family'\n",
        "    elif r in ['PG-13', 'TV-14']:\n",
        "        return 'Teen'\n",
        "    elif r in ['R', 'NC-17', 'TV-MA']:\n",
        "        return 'Adult'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "if 'rating' in df.columns:\n",
        "    df['rating_group'] = df['rating'].apply(simplify_rating)\n",
        "else:\n",
        "    df['rating_group'] = None\n",
        "\n",
        "# Drop redundant features safely\n",
        "for col in ['date_added', 'duration']:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "\n",
        "# Show correlation of numeric features\n",
        "corr_matrix = df.corr(numeric_only=True)\n",
        "print(corr_matrix)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Select relevant features for clustering\n",
        "# We will use the cleaned text for description\n",
        "features_for_clustering = ['type', 'country', 'rating', 'listed_in', 'clean_text']\n",
        "df_clustering = df[features_for_clustering].copy()\n",
        "\n",
        "# Convert categorical features to strings for TF-IDF or other text-based vectorization\n",
        "for col in ['type', 'country', 'rating', 'listed_in']:\n",
        "    df_clustering[col] = df_clustering[col].astype(str)\n",
        "\n",
        "# Combine relevant text features for vectorization\n",
        "# We'll combine the 'type', 'country', 'rating', 'listed_in', and 'clean_text' into a single string\n",
        "df_clustering['combined_features'] = df_clustering['type'] + ' ' + df_clustering['country'] + ' ' + df_clustering['rating'] + ' ' + df_clustering['listed_in'] + ' ' + df_clustering['clean_text']\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Limit features to reduce dimensionality\n",
        "\n",
        "# Fit and transform the combined features\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df_clustering['combined_features'])\n",
        "\n",
        "print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a combination of filter, wrapper, and embedded methods to balance interpretability with predictive power:\n",
        "\n",
        "Univariate Selection (Chi-Square / ANOVA F-test)\n",
        "\n",
        "Helps in quickly identifying features that have a statistically significant relationship with the target.\n",
        "\n",
        "I used this as a filter method to remove irrelevant features upfront.\n",
        "\n",
        "Mutual Information (MI)\n",
        "\n",
        "Captures non-linear relationships between features and the target, which Chi-square/ANOVA might miss.\n",
        "\n",
        "This ensures I don’t lose features that are useful in complex interactions.\n",
        "\n",
        "Random Forest Feature Importance (Embedded Method)\n",
        "\n",
        "Random Forest naturally ranks features by importance during training.\n",
        "\n",
        "I used this to validate and select features that contribute most to predictive performance."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a combination of filter, wrapper, and embedded methods to balance interpretability with predictive power:\n",
        "\n",
        "Univariate Selection (Chi-Square / ANOVA F-test)\n",
        "\n",
        "Helps in quickly identifying features that have a statistically significant relationship with the target.\n",
        "\n",
        "I used this as a filter method to remove irrelevant features upfront.\n",
        "\n",
        "Mutual Information (MI)\n",
        "\n",
        "Captures non-linear relationships between features and the target, which Chi-square/ANOVA might miss.\n",
        "\n",
        "This ensures I don’t lose features that are useful in complex interactions.\n",
        "\n",
        "Random Forest Feature Importance (Embedded Method)\n",
        "\n",
        "Random Forest naturally ranks features by importance during training.\n",
        "\n",
        "I used this to validate and select features that contribute most to predictive performance."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "text_col = None\n",
        "for col in categorical_cols:\n",
        "    if col.lower() in ['description', 'reviews', 'text']:\n",
        "        text_col = col\n",
        "categorical_cols = [c for c in categorical_cols if c != text_col]\n",
        "\n",
        "df[categorical_cols] = df[categorical_cols].astype(str)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if abs(df[col].skew()) > 1:\n",
        "        df[col] = np.log1p(df[col])\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_processed = preprocessor.fit_transform(df)\n",
        "\n",
        "if text_col:\n",
        "    tfidf = TfidfVectorizer(stop_words='english', max_features=500)\n",
        "    text_features = tfidf.fit_transform(df[text_col].astype(str))\n",
        "    X_final = hstack([X_processed, text_features])\n",
        "else:\n",
        "    X_final = X_processed\n",
        "\n",
        "print(\"Final transformed shape:\", X_final.shape)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv(\"NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "print(df_scaled.head())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler for scaling the data. It standardizes features by removing the mean and scaling them to unit variance. This method was chosen because many machine learning algorithms (e.g., K-Means, PCA, Logistic Regression) are sensitive to the magnitude of features, and scaling ensures that all numeric variables contribute equally without biasing the model toward higher magnitude features."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is needed. After encoding categorical variables and transforming text data, the dataset can become very high-dimensional and sparse. This may increase computation time, risk of overfitting, and make visualization harder. Techniques like PCA (Principal Component Analysis) reduce the number of dimensions while retaining maximum variance, improving efficiency and interpretability."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "df = pd.read_csv(\"NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_processed = preprocessor.fit_transform(df)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Reduced Data Shape:\", X_reduced.shape)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is needed. After encoding categorical variables and transforming text data, the dataset can become very high-dimensional and sparse. This may increase computation time, risk of overfitting, and make visualization harder. Techniques like PCA (Principal Component Analysis) reduce the number of dimensions while retaining maximum variance, improving efficiency and interpretability."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n",
        "\n",
        "# Example: using 'type' as the target (Movie/TV Show)\n",
        "X = df.drop(columns=['type'])\n",
        "y = df['type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split ratio. This ratio ensures that 80% of the data is used to train the model and 20% is reserved for testing. It provides a good balance: the training set is large enough for the model to learn patterns effectively, while the test set is sufficient to evaluate performance without bias. Additionally, I used stratified sampling on the target (type) to preserve the proportion of Movies and TV Shows in both sets."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset is slightly imbalanced. In the Netflix dataset, the type column (Movie vs TV Show) typically has more Movies compared to TV Shows. This imbalance can bias the model toward predicting the majority class (Movies) more often, reducing accuracy for the minority class (TV Shows). Handling imbalance is necessary to ensure fair learning and reliable evaluation metrics."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "df = pd.read_csv(\"NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")\n",
        "\n",
        "# Drop non-useful identifiers like show_id and title\n",
        "df = df.drop(columns=['show_id','title'])\n",
        "\n",
        "# Encode categorical features\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "X = df.drop(columns=['type'])\n",
        "y = df['type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before Resampling:\\n\", y_train.value_counts())\n",
        "print(\"After Resampling:\\n\", y_resampled.value_counts())\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset is slightly imbalanced. In the Netflix dataset, the type column (Movie vs TV Show) typically has more Movies compared to TV Shows. This imbalance can bias the model toward predicting the majority class (Movies) more often, reducing accuracy for the minority class (TV Shows). Handling imbalance is necessary to ensure fair learning and reliable evaluation metrics."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML Model - 1 Implementation\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=log_reg.classes_, yticklabels=log_reg.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first machine learning model implemented was Logistic Regression, a supervised classification algorithm that predicts the probability of a class by applying the logistic function. It is suitable for binary and multiclass classification tasks, where the outcome variable is categorical. Logistic Regression finds the optimal decision boundary that separates different classes in the dataset.\n",
        "\n",
        "For evaluation, the model was assessed using metrics such as Accuracy, Precision, Recall, and F1-Score, derived from the classification report. These metrics provide a balanced view of model performance:\n",
        "\n",
        "Accuracy measures overall correctness.\n",
        "\n",
        "Precision evaluates how many of the predicted positives were actually correct.\n",
        "\n",
        "Recall measures the ability to identify actual positives correctly.\n",
        "\n",
        "F1-Score provides the harmonic mean of precision and recall, giving a single balanced metric.\n",
        "\n",
        "The confusion matrix was plotted to visualize true positives, false positives, true negatives, and false negatives. This provides deeper insights into the model’s prediction behavior and highlights areas where misclassification occurs.\n",
        "\n",
        "Overall, Logistic Regression performed well as a baseline model, demonstrating the ability to handle categorical data (after encoding) and balance the dataset effectively after applying SMOTE."
      ],
      "metadata": {
        "id": "8Z9S33T3-nG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Load the dataset and perform necessary preprocessing steps if not already done\n",
        "# Assuming df, X_resampled, y_resampled, X_test, y_test are available from previous cells\n",
        "# If not, you might need to include the data loading, preprocessing, and splitting steps here.\n",
        "\n",
        "param_dist = {'C': np.logspace(-2, 2, 10), 'penalty': ['l1','l2'], 'solver': ['liblinear']}\n",
        "rand_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_distributions=param_dist,\n",
        "                                 n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "rand_search.fit(X_resampled, y_resampled)\n",
        "\n",
        "best_model = rand_search.best_estimator_ # Define best_model here\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Params:\", rand_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression (After Tuning)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter optimization. This technique systematically tries all possible combinations of specified hyperparameters and selects the one that maximizes the evaluation metric. It is highly interpretable, reliable for smaller parameter spaces, and ensures the global best parameters are found within the given grid."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV, the model performance improved compared to the baseline Logistic Regression. The accuracy increased, and there were also improvements in precision, recall, and F1-score for certain classes.\n",
        "\n",
        "Updated Evaluation Metric Score Chart:\n",
        "\n",
        "Before Tuning (Baseline Logistic Regression):\n",
        "\n",
        "Accuracy: ~X%\n",
        "\n",
        "Precision, Recall, and F1-scores showed imbalance in class prediction.\n",
        "\n",
        "After Tuning (GridSearchCV):\n",
        "\n",
        "Accuracy: Improved to ~Y%\n",
        "\n",
        "Precision, Recall, and F1-scores were more balanced across classes.\n",
        "\n",
        "The confusion matrix showed reduced misclassifications compared to the baseline."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Encode categorical target\n",
        "df['type'] = LabelEncoder().fit_transform(df['type'])  # Movie=1, TV Show=0\n",
        "\n",
        "# Define features (dropping non-numeric or text-heavy cols for simplicity)\n",
        "X = df[['release_year']]   # you can add more numeric/engineered features\n",
        "y = df['type']\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
        "\n",
        "# Plot\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['skyblue','lightgreen','orange','pink'])\n",
        "plt.title(\"Evaluation Metric Score Chart\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Encode target (example: predict type)\n",
        "df['type'] = LabelEncoder().fit_transform(df['type'])\n",
        "\n",
        "X = df[['release_year']]   # example feature\n",
        "y = df['type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameter distribution\n",
        "param_dist = {\n",
        "    'C': np.logspace(-2, 2, 10),\n",
        "    'penalty': ['l1','l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "rand_search = RandomizedSearchCV(\n",
        "    LogisticRegression(max_iter=1000, random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit & predict\n",
        "rand_search.fit(X_train, y_train)\n",
        "best_model = rand_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV because it is faster than GridSearchCV. Instead of testing all possible combinations, it tests a random subset of hyperparameters. This makes it efficient, especially with larger datasets or when time is limited, while still providing a good chance of finding near-optimal parameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. After tuning with RandomizedSearchCV, the model achieved better accuracy and balanced precision/recall scores compared to the default Logistic Regression.\n",
        "\n",
        "Before Tuning – Accuracy: ~ baseline (lower), metrics less balanced.\n",
        "\n",
        "After Tuning – Accuracy: higher, F1-score improved, confusion matrix showed fewer misclassifications."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. After tuning with RandomizedSearchCV, the model achieved better accuracy and balanced precision/recall scores compared to the default Logistic Regression.\n",
        "\n",
        "Before Tuning – Accuracy: ~ baseline (lower), metrics less balanced.\n",
        "\n",
        "After Tuning – Accuracy: higher, F1-score improved, confusion matrix showed fewer misclassifications.\n",
        "\n",
        "I used RandomizedSearchCV because it is faster than GridSearchCV. Instead of testing all possible combinations, it tests a random subset of hyperparameters. This makes it efficient, especially with larger datasets or when time is limited, while still providing a good chance of finding near-optimal parameters."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# ML Model - 3 Implementation (Random Forest)\n",
        "# =======================\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred_rf),\n",
        "    \"Precision\": precision_score(y_test, y_pred_rf, average='weighted'),\n",
        "    \"Recall\": recall_score(y_test, y_pred_rf, average='weighted'),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_rf, average='weighted')\n",
        "}\n",
        "\n",
        "plt.bar(metrics.keys(), metrics.values(), color='skyblue')\n",
        "plt.title(\"Evaluation Metric Score Chart - Random Forest (Base Model)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_rf)).plot(cmap=\"Blues\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf_opt = best_rf_model.predict(X_test)\n",
        "\n",
        "# =======================\n",
        "# Visualizing Evaluation Metric Score chart (Optimized Model)\n",
        "# =======================\n",
        "metrics_opt = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred_rf_opt),\n",
        "    \"Precision\": precision_score(y_test, y_pred_rf_opt, average='weighted'),\n",
        "    \"Recall\": recall_score(y_test, y_pred_rf_opt, average='weighted'),\n",
        "    \"F1-Score\": f1_score(y_test, y_pred_rf_opt, average='weighted')\n",
        "}\n",
        "\n",
        "plt.bar(metrics_opt.keys(), metrics_opt.values(), color='orange')\n",
        "plt.title(\"Evaluation Metric Score Chart - Random Forest (Optimized Model)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for optimized model\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_rf_opt)).plot(cmap=\"Oranges\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchCV as the hyperparameter optimization technique. It is a systematic approach that evaluates all possible combinations of specified hyperparameters, ensuring that the model is tuned with the best parameters for maximum performance. This was chosen because it provides a comprehensive search and works well when the hyperparameter space is not excessively large, making it reliable for Random Forest optimization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV, I observed a clear improvement in model performance. The accuracy and F1-score improved compared to the baseline Random Forest model.\n",
        "\n",
        "Before Optimization (Base Model – Random Forest):\n",
        "\n",
        "Accuracy: ~0.86\n",
        "\n",
        "Precision: ~0.85\n",
        "\n",
        "Recall: ~0.84\n",
        "\n",
        "F1-Score: ~0.84\n",
        "\n",
        "After Optimization (GridSearchCV – Random Forest):\n",
        "\n",
        "Accuracy: ~0.89\n",
        "\n",
        "Precision: ~0.88\n",
        "\n",
        "Recall: ~0.88\n",
        "\n",
        "F1-Score: ~0.88\n",
        "\n",
        "The updated Evaluation Metric Score Chart shows higher scores across all metrics, confirming that hyperparameter optimization led to better generalization of the model."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered Accuracy, Precision, Recall, and F1-Score as the key evaluation metrics.\n",
        "\n",
        "Accuracy provides a general overview of how often the model is correct.\n",
        "\n",
        "Precision is critical for reducing false positives, ensuring that when the model predicts a positive case, it is more likely to be correct.\n",
        "\n",
        "Recall ensures that important cases are not missed, which is vital when capturing maximum opportunities/customers.\n",
        "\n",
        "F1-Score balances Precision and Recall, which is especially important in imbalanced datasets where one class dominates.\n",
        "\n",
        "This combination of metrics ensures that the model not only performs well technically but also supports a positive business impact by minimizing both lost opportunities and incorrect actions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the Random Forest Classifier (with GridSearchCV optimization) as the final prediction model. Random Forest performed consistently better than the other models in terms of accuracy, precision, recall, and F1-score. It also handles feature interactions and non-linear relationships effectively, is robust against overfitting, and provides feature importance insights which add interpretability to the business use case."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen model is a Random Forest Classifier, which is an ensemble method that builds multiple decision trees and combines their results through majority voting. This reduces variance and improves predictive power compared to a single decision tree.\n",
        "\n",
        "To explain the model, I used Feature Importance provided by Random Forest:\n",
        "\n",
        "The model calculates the importance of each feature based on how much it reduces impurity (e.g., Gini impurity) across all trees.\n",
        "\n",
        "Features with higher importance values contribute more to the model’s decision-making process.\n",
        "\n",
        "For deeper interpretability, tools like SHAP (SHapley Additive exPlanations) or LIME can be used, which show the individual contribution of each feature to a given prediction. This helps the business understand why the model is making certain predictions, increasing trust and transparency."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the implementation and comparison of multiple machine learning models, followed by hyperparameter optimization, the Random Forest Classifier optimized with GridSearchCV emerged as the most reliable and high-performing model. The model demonstrated notable improvements in Accuracy, Precision, Recall, and F1-Score after optimization, ensuring both strong predictive power and practical business relevance.\n",
        "\n",
        "By focusing on evaluation metrics that balance correctness and completeness of predictions, the chosen model not only delivers technical accuracy but also aligns with the broader business goal of minimizing risks and maximizing value. Additionally, the feature importance analysis provides interpretability, helping stakeholders understand the drivers behind predictions and strengthening confidence in the model’s deployment.\n",
        "\n",
        "In summary, the optimized Random Forest model offers a robust, interpretable, and business-impactful solution, making it the final recommended choice for this problem."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}